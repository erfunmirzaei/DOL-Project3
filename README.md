# DOL-Project3
Compared Non-stationary Multi-armed Bandits in Single-Agent to Multi-Agents Scenarios- Distributed Optimization and Learning(DOL) Course Project

## Overview

We implement Bandit learning algorithms for single-agent and multi-agent scenarios in this project. To this end, we used a non-stationary environment where some reward functions change disruptively.

 We consider different single-agent multi-armed bandits with 2 and 10 arms in the first part. In both cases, we design environments with different difficulty levels, i.e., discriminability of rewards. We use Epsilon-greedy, Upper-Confidence Bound(UCB), Policy-gradient, Thompson Sampling, and Actor-Critic algorithms in this part. 

In the second part, we consider multi-agent multi-armed bandit scenarios. Similar to the former, we consider different numbers of arms with different reward probability distributions. In this part, we use Joint Action Learners(JAL), Free Maximum Q-value(FMQ), Distributed Q-learning, and Multi-agent Actor-Critic Algorithm.  
